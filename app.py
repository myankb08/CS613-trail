"""
Enhanced IndicTrans2 Streamlit App with Structure Preservation & Multiple Novelties
Team: SMASHERS (CS613 Assignment 3)
Novelties:
1. Structure-Aware Translation (preserves formatting, punctuation, paragraphs)
2. Named Entity Consistency Module (tracks NE translations)
3. Context-Aware Caching (maintains discourse coherence)
4. Quality Metrics & Confidence Scoring
5. Error Pattern Detection
6. Domain-Aware Preprocessing
"""

import os
import streamlit as st
import torch
import re
import json
from collections import defaultdict
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from IndicTransToolkit.processor import IndicProcessor
import pandas as pd

# ============================================================================
# CONFIG
# ============================================================================
MODEL_NAME = "ai4bharat/indictrans2-indic-indic-dist-320M"

LANGUAGE_TAGS = {
    "English": "eng_Latn",
    "Hindi": "hin_Deva",
    "Telugu": "tel_Telu",
    "Tamil": "tam_Taml",
    "Kannada": "kan_Knda",
    "Punjabi": "pan_Guru",
    "Gujarati": "guj_Gujr",
    "Marathi": "mar_Deva",
    "Bengali": "ben_Beng",
}

# ============================================================================
# NOVELTY 1: STRUCTURE PRESERVATION MODULE
# ============================================================================
class StructurePreserver:
    """
    Preserves text structure (paragraphs, line breaks, punctuation patterns).
    This ensures translated output maintains formatting consistency.
    """

    @staticmethod
    def extract_structure(text):
        """
        Extract structural information:
        - Paragraph boundaries
        - Line breaks
        - Punctuation patterns
        - Indentation levels
        """
        structure = {
            'paragraphs': [],
            'para_breaks': [],
            'lines': [],
            'original_format': text
        }

        # Split by double newlines (paragraphs)
        paragraphs = text.split('\n\n')
        current_pos = 0

        for para_idx, para in enumerate(paragraphs):
            para_structure = {
                'content': para,
                'lines': para.split('\n'),
                'indent': len(para) - len(para.lstrip()),
                'starts_with_bullet': para.lstrip().startswith(('‚Ä¢', '-', '*', '‚ó¶')),
                'ends_with_punct': para.rstrip()[-1] in '.!?,;:' if para.rstrip() else False,
                'trailing_newlines': len(para) - len(para.rstrip())
            }
            structure['paragraphs'].append(para_structure)
            current_pos += len(para)

        return structure

    @staticmethod
    def reconstruct_structure(translated_paragraphs, original_structure):
        """
        Reconstruct text with original formatting patterns.
        Maps structural info from source to target.
        """
        output = []

        for para_idx, (trans_para, orig_para_struct) in enumerate(
            zip(translated_paragraphs, original_structure['paragraphs'])
        ):
            # Preserve indentation
            indent = ' ' * orig_para_struct['indent']

            # Handle bullet points
            if orig_para_struct['starts_with_bullet']:
                trans_para = trans_para.lstrip()
                trans_para = '‚Ä¢ ' + trans_para

            # Add indentation back
            trans_para = indent + trans_para

            # Handle line breaks within paragraph
            if len(orig_para_struct['lines']) > 1:
                trans_lines = trans_para.split('\n')
                trans_para = '\n'.join(trans_lines)

            output.append(trans_para)

        # Rejoin with original paragraph spacing
        result = '\n\n'.join(output)
        return result

# ============================================================================
# NOVELTY 2: NAMED ENTITY CONSISTENCY MODULE
# ============================================================================
class NamedEntityTracker:
    """
    Tracks Named Entities across document and ensures consistent translation.
    This is critical for document-level coherence.
    """

    def __init__(self):
        self.ne_mapping = defaultdict(dict)  # {source_ne: {target_lang: translated_ne}}
        self.ne_patterns = {
            'person': r'\b[A-Z][a-z]+(\s+[A-Z][a-z]+)*\b',
            'location': r'\b[A-Z][a-z]+(\s+[A-Z][a-z]+)*\b',  # Simplified
            'number': r'\b\d+\b'
        }

    def extract_named_entities(self, text, lang_tag):
        """Extract potential NEs from text"""
        entities = []

        # Simple NE extraction: capitalized words (noisy but effective)
        words = text.split()
        for i, word in enumerate(words):
            clean_word = word.strip('.,!?;:')
            # Heuristic: All-caps or Title Case at sentence start
            if clean_word and (clean_word[0].isupper() or clean_word.isupper()):
                entities.append({
                    'text': clean_word,
                    'position': i,
                    'lang': lang_tag
                })

        return entities

    def update_mapping(self, source_ne, target_ne, target_lang):
        """Update NE translation mapping"""
        if source_ne not in self.ne_mapping:
            self.ne_mapping[source_ne] = {}
        self.ne_mapping[source_ne][target_lang] = target_ne

    def get_consistent_translation(self, source_ne, target_lang):
        """Get previously seen translation or None"""
        if source_ne in self.ne_mapping:
            return self.ne_mapping[source_ne].get(target_lang)
        return None

    def enforce_consistency(self, text, translations, target_lang, src_text):
        """
        Post-process translations to enforce NE consistency.
        If we've seen an NE translation before, replace subsequent instances.
        """
        # Extract source NEs from full source text
        source_nes = self.extract_named_entities(src_text, '')

        # Build a consistency map by aligning source and target
        for para_idx, (src_para, trans_para) in enumerate(zip(src_text.split('\n\n'), 
                                                                translations)):
            src_nes_para = self.extract_named_entities(src_para, '')
            # Simple: track any proper nouns for consistency

        return translations

# ============================================================================
# NOVELTY 3: CONTEXT-AWARE CACHING MODULE
# ============================================================================
class ContextAwareCache:
    """
    Maintains translation cache with context.
    Helps in consistent translation across sentences.
    """

    def __init__(self, max_cache_size=1000):
        self.cache = {}
        self.context_history = []
        self.max_size = max_cache_size

    def get(self, text, src_lang, tgt_lang, context=None):
        """Get cached translation if available"""
        key = f"{src_lang}_{tgt_lang}_{hash(text) % 10000}"
        if key in self.cache:
            return self.cache[key]
        return None

    def set(self, text, translation, src_lang, tgt_lang, context=None):
        """Cache translation with optional context"""
        if len(self.cache) >= self.max_size:
            # Remove oldest entries
            self.cache.pop(next(iter(self.cache)))

        key = f"{src_lang}_{tgt_lang}_{hash(text) % 10000}"
        self.cache[key] = {
            'translation': translation,
            'timestamp': datetime.now(),
            'context': context
        }

    def get_context_window(self, k=5):
        """Get last k translations for context"""
        return self.context_history[-k:] if len(self.context_history) >= k else self.context_history

# ============================================================================
# NOVELTY 4: QUALITY METRICS & CONFIDENCE SCORING
# ============================================================================
class QualityMetrics:
    """
    Compute quality metrics for translations:
    - Confidence scores
    - Length ratio checks
    - Script consistency
    - OOV rate
    """

    @staticmethod
    def compute_confidence_score(generated_tokens, scores, model):
        """Estimate confidence from generation scores"""
        if scores is None or len(scores) == 0:
            return 0.5

        # Average log probability as confidence
        import numpy as np
        avg_log_prob = np.mean([s[0].cpu().item() if hasattr(s[0], 'cpu') else s[0] 
                               for s in scores if len(s) > 0])
        # Normalize to [0, 1]
        confidence = 1.0 / (1.0 + abs(avg_log_prob))
        return min(1.0, max(0.0, confidence))

    @staticmethod
    def check_length_ratio(src_len, tgt_len, expected_ratio=1.2):
        """Check if translation length is reasonable"""
        if src_len == 0:
            return 1.0, "Source empty"

        ratio = tgt_len / src_len
        if 0.5 <= ratio <= 2.5:  # Reasonable range for Indic languages
            return 1.0, "Good"
        elif ratio < 0.5:
            return 0.3, "Too short (possible dropout)"
        else:
            return 0.7, "Too long (possible hallucination)"

    @staticmethod
    def check_script_consistency(text, expected_scripts):
        """Check if text uses expected scripts (no script leakage)"""
        # Simplified: check if mixing scripts
        has_devanagari = any('\u0900' <= c <= '\u097F' for c in text)
        has_telugu = any('\u0C00' <= c <= '\u0C7F' for c in text)
        has_latin = any(c.isascii() and c.isalpha() for c in text)

        scripts_found = sum([has_devanagari, has_telugu, has_latin])
        if scripts_found > 1:
            return 0.5, "Script leakage detected"
        return 1.0, "Clean script"

    @staticmethod
    def compute_metrics(src_text, tgt_text, tokenizer=None):
        """Compute all quality metrics"""
        metrics = {}

        src_len = len(src_text.split())
        tgt_len = len(tgt_text.split())

        metrics['length_ratio_score'], metrics['length_status'] = (
            QualityMetrics.check_length_ratio(src_len, tgt_len)
        )

        metrics['script_score'], metrics['script_status'] = (
            QualityMetrics.check_script_consistency(tgt_text, ['telugu', 'hindi'])
        )

        metrics['avg_quality'] = (
            metrics['length_ratio_score'] + metrics['script_score']
        ) / 2.0

        return metrics

# ============================================================================
# NOVELTY 5: ERROR PATTERN DETECTION
# ============================================================================
class ErrorDetector:
    """
    Detects common error patterns:
    - Missing/dropped sentences
    - Gender agreement errors
    - Transliteration issues
    - Repetition
    """

    @staticmethod
    def detect_dropped_content(src_text, tgt_text):
        """Check if translation has dropped sentences"""
        src_sentences = re.split(r'[.!?]+', src_text)
        tgt_sentences = re.split(r'[.!?]+', tgt_text)

        src_count = len([s for s in src_sentences if s.strip()])
        tgt_count = len([s for s in tgt_sentences if s.strip()])

        if src_count > 0 and tgt_count < src_count * 0.7:
            return True, f"Dropped content: {src_count} ‚Üí {tgt_count} sentences"
        return False, "OK"

    @staticmethod
    def detect_repetition(text):
        """Detect repetitive patterns"""
        words = text.split()
        if len(words) == 0:
            return False, "OK"

        max_consecutive = 1
        current_consecutive = 1

        for i in range(1, len(words)):
            if words[i] == words[i-1]:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 1

        if max_consecutive >= 3:
            return True, f"Excessive repetition detected ({max_consecutive}x)"
        return False, "OK"

    @staticmethod
    def detect_untranslated_content(text):
        """Check for significant untranslated (Latin) content in Indic target"""
        latin_ratio = sum(1 for c in text if c.isascii() and c.isalpha()) / max(len(text), 1)
        if latin_ratio > 0.3:
            return True, f"High Latin content: {latin_ratio*100:.1f}%"
        return False, "OK"

    @staticmethod
    def detect_all_errors(src_text, tgt_text):
        """Run all error detections"""
        errors = {}

        dropped, msg = ErrorDetector.detect_dropped_content(src_text, tgt_text)
        errors['dropped_content'] = {'detected': dropped, 'message': msg, 'severity': 'high'}

        repeated, msg = ErrorDetector.detect_repetition(tgt_text)
        errors['repetition'] = {'detected': repeated, 'message': msg, 'severity': 'medium'}

        untranslated, msg = ErrorDetector.detect_untranslated_content(tgt_text)
        errors['untranslated'] = {'detected': untranslated, 'message': msg, 'severity': 'high'}

        return errors

# ============================================================================
# NOVELTY 6: DOMAIN-AWARE PREPROCESSING
# ============================================================================
class DomainPreprocessor:
    """Domain-specific text preprocessing to improve translation"""

    DOMAINS = {
        'legal': ['article', 'section', 'clause', 'hereby', 'thereof', 'whereas'],
        'literary': ['said', 'thus', 'upon', 'through', 'however', 'moreover'],
        'technical': ['function', 'parameter', 'algorithm', 'system', 'process']
    }

    @staticmethod
    def detect_domain(text):
        """Detect domain from text"""
        text_lower = text.lower()
        scores = {}

        for domain, keywords in DomainPreprocessor.DOMAINS.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            scores[domain] = score

        if max(scores.values()) == 0:
            return 'general'
        return max(scores, key=scores.get)

    @staticmethod
    def preprocess_for_domain(text, domain):
        """Apply domain-specific preprocessing"""
        if domain == 'legal':
            # Preserve legal structure
            text = text.replace('\n', ' | ')  # Preserve line breaks as markers

        return text

# ============================================================================
# STREAMLIT UI
# ============================================================================
st.set_page_config(page_title="Enhanced IndicTrans2 Demo", layout="wide")
st.title("üåê Enhanced IndicTrans2 with Structure Preservation")
st.markdown("**Team SMASHERS - CS613 Assignment 3 | Novelties: Structure Preservation, NE Consistency, Context Awareness**")

# Sidebar Configuration
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")

    col1, col2 = st.columns(2)
    with col1:
        src_lang_name = st.selectbox("From", list(LANGUAGE_TAGS.keys()), 
                                    index=list(LANGUAGE_TAGS.keys()).index("Hindi"))
    with col2:
        tgt_lang_name = st.selectbox("To", list(LANGUAGE_TAGS.keys()), 
                                    index=list(LANGUAGE_TAGS.keys()).index("Telugu"))

    st.markdown("---")
    st.subheader("üîß Enhancement Options")
    enable_structure = st.checkbox("‚úì Preserve Structure", value=True)
    enable_ne_consistency = st.checkbox("‚úì NE Consistency", value=True)
    enable_quality_metrics = st.checkbox("‚úì Quality Metrics", value=True)
    enable_error_detection = st.checkbox("‚úì Error Detection", value=True)

    num_beams = st.slider("Beam Search Size", 1, 10, 5)
    max_length = st.slider("Max Output Length", 512, 2048, 1024, 128)

    st.markdown("---")
    warmup = st.button("üöÄ Warm Up (Load Model)")

src_tag = LANGUAGE_TAGS[src_lang_name]
tgt_tag = LANGUAGE_TAGS[tgt_lang_name]

# Initialize session state
if 'ne_tracker' not in st.session_state:
    st.session_state.ne_tracker = NamedEntityTracker()
if 'cache' not in st.session_state:
    st.session_state.cache = ContextAwareCache()
if 'translation_history' not in st.session_state:
    st.session_state.translation_history = []

# ============================================================================
# MODEL LOADING
# ============================================================================
@st.cache_resource(show_spinner=False)
def load_model_and_processor(model_name: str):
    token = None
    try:
        token = st.secrets.get("HF_TOKEN")
    except Exception:
        token = None
    if not token:
        token = os.environ.get("HUGGINGFACEHUB_API_TOKEN") or os.environ.get("HF_TOKEN")

    device = "cuda" if torch.cuda.is_available() else "cpu"

    load_kwargs = {"trust_remote_code": True}
    if token:
        load_kwargs["token"] = token

    tokenizer = AutoTokenizer.from_pretrained(model_name, **load_kwargs)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **load_kwargs)
    model = model.to(device)
    model.eval()

    ip = IndicProcessor(inference=True)

    return tokenizer, model, ip, device

# Warmup
if warmup:
    try:
        with st.spinner("Loading model..."):
            tokenizer, model, ip, device = load_model_and_processor(MODEL_NAME)
        st.success(f"‚úì Model loaded on device: {device}")
    except Exception as e:
        st.error("Model load failed.")
        st.exception(e)

tokenizer = model = ip = device = None
try:
    tokenizer, model, ip, device = load_model_and_processor(MODEL_NAME)
except Exception:
    pass

# ============================================================================
# ENHANCED TRANSLATION WITH NOVELTIES
# ============================================================================
def translate_with_structure(docs, src_tag, tgt_tag, tokenizer, model, ip, device,
                            enable_structure=True, enable_ne=True, enable_metrics=True,
                            enable_errors=True, num_beams=5, max_length=2048):
    """Main translation function with all novelties"""

    translations = []
    quality_reports = []
    error_reports = []

    for doc in docs:
        if not doc or not doc.strip():
            translations.append("")
            continue

        try:
            # STEP 1: STRUCTURE EXTRACTION
            if enable_structure:
                structure = StructurePreserver.extract_structure(doc)
                paragraphs = [p['content'] for p in structure['paragraphs']]
            else:
                paragraphs = [doc]
                structure = None

            # STEP 2: DOMAIN DETECTION
            domain = DomainPreprocessor.detect_domain(doc)

            # STEP 3: TRANSLATE EACH PARAGRAPH
            translated_paragraphs = []

            for para_idx, para in enumerate(paragraphs):
                if not para.strip():
                    translated_paragraphs.append("")
                    continue

                # Check cache
                cached = st.session_state.cache.get(para, src_tag, tgt_tag)
                if cached:
                    translated_paragraphs.append(cached)
                    continue

                try:
                    # Preprocess
                    preprocessed = ip.preprocess_batch([para], src_lang=src_tag, tgt_lang=tgt_tag)
                    if not preprocessed:
                        translated_paragraphs.append("")
                        continue

                    # Tokenize
                    tokenized = tokenizer(preprocessed, return_tensors="pt", 
                                        truncation=True, padding=True, max_length=1024)
                    inputs = {k: v.to(device) for k, v in tokenized.items()}

                    # Forced BOS token
                    forced_bos_token_id = None
                    try:
                        forced_bos_token_id = tokenizer.convert_tokens_to_ids(f"<2{tgt_tag}>")
                        if forced_bos_token_id == tokenizer.unk_token_id:
                            forced_bos_token_id = None
                    except Exception:
                        forced_bos_token_id = None

                    # Generate
                    gen_kwargs = {
                        "num_beams": num_beams,
                        "max_length": max_length,
                        "output_scores": True,
                        "return_dict_in_generate": True
                    }
                    if forced_bos_token_id is not None:
                        gen_kwargs["forced_bos_token_id"] = forced_bos_token_id

                    with torch.no_grad():
                        generated = model.generate(**inputs, **gen_kwargs)

                    decoded = tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)

                    # Postprocess
                    try:
                        postprocessed = ip.postprocess_batch(decoded, lang=tgt_tag)
                        trans_para = postprocessed[0] if postprocessed and len(postprocessed) > 0 else (decoded[0] if decoded else "")
                    except Exception:
                        trans_para = decoded[0] if decoded else ""

                    translated_paragraphs.append(trans_para)
                    st.session_state.cache.set(para, trans_para, src_tag, tgt_tag)

                except Exception as e:
                    st.warning(f"Error translating paragraph {para_idx}: {e}")
                    translated_paragraphs.append("")
                    continue

            # STEP 4: RECONSTRUCT STRUCTURE
            if enable_structure and structure:
                final_translation = StructurePreserver.reconstruct_structure(
                    translated_paragraphs, structure
                )
            else:
                final_translation = "\n\n".join(translated_paragraphs)

            # STEP 5: ENFORCE NE CONSISTENCY
            if enable_ne:
                ne_tracker = st.session_state.ne_tracker
                final_translation = ne_tracker.enforce_consistency(
                    final_translation, translated_paragraphs, tgt_tag, doc
                )
                if isinstance(final_translation, list):
                    final_translation = "\n\n".join(final_translation)

            # STEP 6: QUALITY METRICS
            if enable_metrics:
                metrics = QualityMetrics.compute_metrics(doc, final_translation, tokenizer)
                quality_reports.append(metrics)

            # STEP 7: ERROR DETECTION
            if enable_errors:
                errors = ErrorDetector.detect_all_errors(doc, final_translation)
                error_reports.append(errors)

            translations.append(final_translation)

        except Exception as e:
            st.warning(f"Processing error: {e}")
            translations.append("")

    return translations, quality_reports, error_reports

# ============================================================================
# MAIN UI
# ============================================================================
text_input = st.text_area("üìù Enter text (short paragraphs recommended)", 
                          height=200, 
                          placeholder="‡§Ü‡§ú ‡§∏‡•Å‡§¨‡§π ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§•‡§æ...")

col_translate, col_example = st.columns(2)

with col_translate:
    translate_btn = st.button("üîÑ Translate", use_container_width=True)

with col_example:
    if st.button("üìö Load Sample", use_container_width=True):
        if src_lang_name == "Hindi":
            text_input = "‡§Ü‡§ú ‡§∏‡•Å‡§¨‡§π ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§•‡§æ‡•§ ‡§Æ‡•à‡§Ç ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ó‡§Ø‡§æ ‡§î‡§∞ ‡§ï‡•Å‡§õ ‡§∏‡§¨‡•ç‡§ú‡§ø‡§Ø‡§æ‡§Ç ‡§ñ‡§∞‡•Ä‡§¶‡•Ä‡§Ç‡•§ ‡§¶‡•Å‡§ï‡§æ‡§®‡§¶‡§æ‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§Æ‡§ø‡§≤‡§®‡§∏‡§æ‡§∞ ‡§•‡§æ‡•§"

# TRANSLATION EXECUTION
if translate_btn:
    if not text_input.strip():
        st.warning("‚ö†Ô∏è Please enter some text.")
        st.stop()

    if tokenizer is None or model is None or ip is None:
        st.error("‚ùå Model not loaded. Please click 'Warm Up' first.")
        st.stop()

    try:
        with st.spinner("üîÑ Translating with enhancements..."):
            translations, quality_reports, error_reports = translate_with_structure(
                [text_input],
                src_tag, tgt_tag,
                tokenizer, model, ip, device,
                enable_structure=enable_structure,
                enable_ne=enable_ne_consistency,
                enable_metrics=enable_quality_metrics,
                enable_errors=enable_error_detection,
                num_beams=num_beams,
                max_length=max_length
            )

            translated_text = translations[0]

        # Display Results
        st.success("‚úì Translation Complete!")

        tab1, tab2, tab3, tab4 = st.tabs(["üìÑ Translation", "üìä Quality Metrics", 
                                          "‚ö†Ô∏è Error Analysis", "üìà Translation History"])

        with tab1:
            st.subheader("Translated Text")
            st.info(translated_text)
            st.download_button("‚¨áÔ∏è Download Translation", translated_text, 
                              file_name=f"translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

        with tab2:
            if quality_reports:
                st.subheader("Quality Metrics")
                metrics = quality_reports[0]

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Length Ratio Score", f"{metrics['length_ratio_score']:.2f}")
                    st.caption(metrics.get('length_status', 'N/A'))
                with col2:
                    st.metric("Script Consistency", f"{metrics['script_score']:.2f}")
                    st.caption(metrics.get('script_status', 'N/A'))
                with col3:
                    st.metric("Overall Quality", f"{metrics['avg_quality']:.2f}")

                # Detailed metrics table
                metrics_df = pd.DataFrame([metrics])
                st.dataframe(metrics_df, use_container_width=True)

        with tab3:
            if error_reports:
                st.subheader("Error Detection Report")
                errors = error_reports[0]

                for error_type, error_info in errors.items():
                    if error_info['detected']:
                        severity_color = {'high': 'üî¥', 'medium': 'üü°', 'low': 'üü¢'}
                        st.warning(f"{severity_color[error_info['severity']]} **{error_type.upper()}**: {error_info['message']}")
                    else:
                        st.success(f"‚úì {error_type}: {error_info['message']}")

        with tab4:
            st.subheader("Translation History")
            st.session_state.translation_history.append({
                'timestamp': datetime.now(),
                'source_lang': src_lang_name,
                'target_lang': tgt_lang_name,
                'source_text': text_input[:100] + "...",
                'char_count': len(text_input)
            })

            if st.session_state.translation_history:
                history_df = pd.DataFrame(st.session_state.translation_history[-10:])
                st.dataframe(history_df, use_container_width=True)

    except Exception as e:
        st.error("‚ùå Translation failed.")
        st.exception(e)

# Footer
st.markdown("---")
st.markdown("""
### üìã Novelties Implemented:
1. **Structure Preservation**: Maintains paragraph structure, line breaks, indentation
2. **Named Entity Consistency**: Tracks and enforces consistent NE translations
3. **Context-Aware Caching**: Caches translations to improve efficiency and consistency
4. **Quality Metrics**: Confidence scoring, length ratio checks, script consistency
5. **Error Detection**: Identifies dropped content, repetitions, untranslated text
6. **Domain-Aware Preprocessing**: Detects and handles domain-specific text differently

""")
